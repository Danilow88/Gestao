#!/usr/bin/env python3
"""
üß™ TESTE DO MATT 2.0 - Sistema de IA H√≠brida

Este arquivo testa as funcionalidades principais do MATT:
1. Conex√£o com Ollama (se dispon√≠vel)
2. Sistema local de fallback
3. Processamento de mensagens
4. Respostas inteligentes

Para executar:
python test_matt.py
"""

import sys
import os

# Adicionar o diret√≥rio app ao path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

def test_matt_basic():
    """Teste b√°sico do sistema MATT"""
    print("üß™ TESTE B√ÅSICO DO MATT 2.0")
    print("=" * 50)
    
    try:
        # Importar fun√ß√µes do dashboard
        from dashboard import process_matt_response, matt_ai_real_response, conectar_ollama_local
        
        # Teste 1: Sistema local
        print("\n1Ô∏è‚É£ Testando Sistema Local...")
        response_local = process_matt_response("oi")
        print(f"‚úÖ Resposta: {response_local[:100]}...")
        
        # Teste 2: Conex√£o Ollama
        print("\n2Ô∏è‚É£ Testando Conex√£o Ollama...")
        try:
            response_ollama = conectar_ollama_local("oi")
            if response_ollama:
                print(f"‚úÖ Ollama conectado! Resposta: {response_ollama[:100]}...")
            else:
                print("‚ö†Ô∏è Ollama n√£o retornou resposta (pode estar offline)")
        except Exception as e:
            print(f"‚ùå Erro na conex√£o Ollama: {str(e)}")
        
        # Teste 3: Sistema h√≠brido
        print("\n3Ô∏è‚É£ Testando Sistema H√≠brido...")
        response_hybrid = matt_ai_real_response("oi")
        print(f"‚úÖ Sistema h√≠brido funcionando! Resposta: {response_hybrid[:100]}...")
        
        # Teste 4: Perguntas espec√≠ficas
        print("\n4Ô∏è‚É£ Testando Perguntas Espec√≠ficas...")
        questions = [
            "Como otimizar meu or√ßamento?",
            "Analise meus dados de perda",
            "Sugira compras para este m√™s",
            "Qual o status do meu estoque?"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\n   Pergunta {i}: {question}")
            response = matt_ai_real_response(question)
            print(f"   Resposta: {response[:150]}...")
        
        print("\nüéâ TODOS OS TESTES CONCLU√çDOS COM SUCESSO!")
        print("‚úÖ O MATT 2.0 est√° funcionando perfeitamente!")
        
    except Exception as e:
        print(f"‚ùå Erro durante os testes: {str(e)}")
        print("üí° Verifique se o arquivo dashboard.py est√° correto")

def test_ollama_connection():
    """Teste espec√≠fico da conex√£o Ollama"""
    print("\nüîß TESTE ESPEC√çFICO DA CONEX√ÉO OLLAMA")
    print("=" * 50)
    
    try:
        import requests
        
        # Teste de conectividade
        print("1Ô∏è‚É£ Testando conectividade com Ollama...")
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        
        if response.status_code == 200:
            print("‚úÖ Ollama est√° rodando e acess√≠vel!")
            
            # Verificar modelos dispon√≠veis
            try:
                models = response.json()
                if 'models' in models:
                    print(f"üì¶ Modelos dispon√≠veis: {len(models['models'])}")
                    for model in models['models']:
                        print(f"   - {model.get('name', 'N/A')}")
                else:
                    print("üì¶ Modelos: Informa√ß√£o n√£o dispon√≠vel")
            except:
                print("üì¶ Modelos: Erro ao processar resposta")
                
        else:
            print(f"‚ùå Ollama retornou status {response.status_code}")
            
    except requests.exceptions.ConnectionError:
        print("‚ùå Ollama n√£o est√° rodando!")
        print("üí° Para ativar Ollama:")
        print("   1. Instale em https://ollama.ai")
        print("   2. Execute: ollama serve")
        print("   3. Execute: ollama pull llama2")
    except Exception as e:
        print(f"‚ùå Erro inesperado: {str(e)}")

def main():
    """Fun√ß√£o principal"""
    print("ü§ñ MATT 2.0 - TESTE COMPLETO DO SISTEMA")
    print("=" * 60)
    
    # Teste b√°sico
    test_matt_basic()
    
    # Teste espec√≠fico Ollama
    test_ollama_connection()
    
    print("\n" + "=" * 60)
    print("üèÅ TESTES FINALIZADOS")
    print("\nüí° DICAS:")
    print("‚Ä¢ Se Ollama n√£o estiver funcionando, o MATT usar√° o sistema local")
    print("‚Ä¢ O sistema local sempre funciona e fornece respostas inteligentes")
    print("‚Ä¢ Para melhor experi√™ncia, configure o Ollama com: ollama pull llama2")
    print("‚Ä¢ Acesse o dashboard em: streamlit run app/dashboard.py")

if __name__ == "__main__":
    main()
